<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-dark.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <title>Silicon Valley AI Lab by svail</title>
  </head>

  <body>
    <div id="container">
      <div class="inner">

        <header>
          <h1>Silicon Valley AI Lab</h1>
          <h2>Baidu Research - <a href="http://research.baidu.com">research.baidu.com</a></h2>
        </header>

        <section id="downloads" class="clearfix">
          <a href="https://github.com/svail" id="view-on-github" class="button"><span>View on GitHub</span></a>
        </section>

        <hr>

        <section id="main_content">
          <h3>
<a id="investigating-performance-of-gpu-blas-libraries" class="anchor" href="#investigating-performance-of-gpu-blas-libraries" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="http://svail.github.io/rnn_perf/">Investigating performance of GPU BLAS Libraries</a>
</h3>

<p><strong>Part I:</strong> Optimizing RNN performance<br>
<strong>Date:</strong> November 17th, 2015<br>
<strong>Author:</strong> Erich Elsen<br>
Most researchers engaging in Neural Network research have been using GPUs for training for some time now due to 
the speed advantage they have over CPUs. GPUs from NVIDIA are almost universally preferred because they come with 
high quality BLAS (cuBLAS) and convolution (cuDNN) libraries. </p>
        
        
            <h3>
<a id="fast-open-source-CPU-GPU-implementation-of-CTC" class="anchor" href="#fast-open-source-CPU-GPU-implementation-of-CTC" aria-hidden="true">
  <span class="octicon octicon-link"></span></a><a href="https://github.com/baidu-research/warp-ctc">Fast Open Source CPU/GPU Implementation of CTC</a>
</h3>

<br>
<p><strong>Date:</strong> January 14th, 2016<br>
Warp-CTC from Baidu Research's Silicon Valley AI Lab is a fast parallel implementation of CTC, on both CPU and GPU. Warp-CTC can be used to solve supervised problems that map an input sequence to an output sequence, such as speech recognition. To get Warp-CTC follow the link above. If you are interested in integrating Warp-CTC into a machine learning framework reach out to us. We are happy to accept pull requests. </p> 
        </section>

        <footer>
          This page was generated by <a href="https://pages.github.com">GitHub Pages</a>. Tactile theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.
        </footer>

                  <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-69560860-3");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

      </div>
    </div>
  </body>
</html>
